import logging
import pickle
import random
from pathlib import Path
from typing import List

import numpy as np
import requests
import typer
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn, MofNCompleteColumn
from rich.table import Table
# --- æ ¸å¿ƒä¿®æ­£ï¼šå¼•å…¥ AutoTokenizer ---
from transformers import AutoTokenizer, set_seed
from datasets import load_from_disk


# --- è®¾ç½® Rich å’Œ Typer ---
logging.basicConfig(
    level="INFO",
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True, markup=True)],
)
app = typer.Typer(pretty_exceptions_show_locals=False)
console = Console()

# --- GPUåŠ é€Ÿæ”¯æŒ ---
try:
    import cupy as cp
    GPU_AVAILABLE = True
    console.print("[green]âœ“ GPUåŠ é€Ÿå·²å¯ç”¨ (CuPy)[/green]")
except ImportError:
    GPU_AVAILABLE = False
    console.print("[yellow]âš  æœªå®‰è£…CuPyï¼Œå°†ä½¿ç”¨CPUè®¡ç®—[/yellow]")
    


# æ‚¨çš„åŸå§‹å‡½æ•°ç­¾åï¼Œä¿æŒä¸å˜
def generate_embeddings_with_tei(dataset, batch_size: int, instruction: str, tei_endpoint: str, tokenizer, max_length: int) -> np.ndarray:
    
    # --- æ–°å¢çš„å¯¼å…¥ ---
    import concurrent.futures
    import threading

    # --- æ–°å¢: ä¸ºå¹¶å‘è¯·æ±‚è®¾ç½®ä¸€ä¸ªçº¿ç¨‹å±€éƒ¨session ---
    # è¿™å¯ä»¥é¿å…å¤šçº¿ç¨‹ç¯å¢ƒä¸‹requests.Sessionçš„æ½œåœ¨é—®é¢˜
    thread_local = threading.local()
    def get_session():
        if not hasattr(thread_local, "session"):
            thread_local.session = requests.Session()
        return thread_local.session

    # --- æ–°å¢: å°†å¾ªç¯å†…çš„é€»è¾‘å°è£…æˆä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•° ---
    # è¿™ä¸ªå‡½æ•°å°†ç”±æ¯ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œï¼Œè´Ÿè´£å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡
    def process_one_batch(batch_texts):
        # è·å–å½“å‰çº¿ç¨‹ä¸“å±çš„session
        session = get_session()
        
        # --- ä¸‹é¢çš„ä»£ç å—ä¸æ‚¨åŸæ¥çš„forå¾ªç¯å†…éƒ¨å®Œå…¨ç›¸åŒ ---
        instructed_texts = [instruction + text for text in batch_texts]
        truncated_inputs = tokenizer(
            instructed_texts,
            truncation=True,
            max_length=max_length,
            padding=False,
        )
        final_texts_to_send = tokenizer.batch_decode(truncated_inputs['input_ids'], skip_special_tokens=True)
        payload = {"inputs": final_texts_to_send}
        
        try:
            response = session.post(f"{tei_endpoint}/embed", json=payload, timeout=60)
            response.raise_for_status()
            return np.array(response.json(), dtype=np.float32)
        except requests.exceptions.RequestException as e:
            # å½“ä¸€ä¸ªè¯·æ±‚å¤±è´¥æ—¶ï¼Œæ‰“å°é”™è¯¯å¹¶é‡æ–°æŠ›å‡ºå¼‚å¸¸
            # executor.mapä¼šæ•è·è¿™ä¸ªå¼‚å¸¸ï¼Œå¹¶åœ¨ä¸»çº¿ç¨‹ä¸­é‡æ–°å¼•å‘å®ƒ
            console.print(f"[bold red]é”™è¯¯: ä¸€ä¸ªå¹¶å‘è¯·æ±‚å¤±è´¥: {e}[/bold red]")
            raise

    # --- ä¿®æ”¹: å°†åŸæ¥çš„forå¾ªç¯æ›¿æ¢ä¸ºThreadPoolExecutor ---

    # 1. é¢„å…ˆå‡†å¤‡å¥½æ‰€æœ‰çš„æ‰¹æ¬¡æ•°æ®
    batches = [dataset[i : i + batch_size]['text'] for i in range(0, len(dataset), batch_size)]
    
    # è®¾ç½®ä¸€ä¸ªåˆç†çš„å¹¶å‘æ•°ï¼Œä¾‹å¦‚8ï¼Œä»¥ç¡®ä¿èƒ½å……åˆ†åˆ©ç”¨2ä¸ªGPU
    # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªå€¼
    MAX_WORKERS = 8 
    all_embeddings = []

    try:
        # 2. ä½¿ç”¨å¹¶å‘æ‰§è¡Œå™¨æ¥å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # executor.mapä¼šè‡ªåŠ¨å¤„ç†å¹¶å‘ï¼Œå¹¶æŒ‰é¡ºåºè¿”å›ç»“æœ
            results_iterator = executor.map(process_one_batch, batches)
            
            # åˆ›å»ºè‡ªå®šä¹‰è¿›åº¦æ¡æ˜¾ç¤ºå®æ—¶é€Ÿåº¦
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TextColumn("â€¢"),
                MofNCompleteColumn(),
                TextColumn("â€¢"),
                TextColumn("[cyan]{task.fields[speed]:.1f} å‡½æ•°/ç§’"),
                TextColumn("â€¢"),
                TimeRemainingColumn(),
            ) as progress:
                task = progress.add_task(
                    f"å¹¶å‘ç”ŸæˆåµŒå…¥(Workers: {MAX_WORKERS})",
                    total=len(dataset),
                    speed=0.0
                )
                
                processed_count = 0
                for batch_emb in results_iterator:
                    all_embeddings.append(batch_emb)
                    processed_count += len(batch_emb)
                    # æ›´æ–°é€Ÿåº¦ï¼šå½“å‰å·²å¤„ç†æ•°é‡ / ç»è¿‡çš„æ—¶é—´
                    elapsed = progress.tasks[0].elapsed or 0.001  # é¿å…é™¤é›¶
                    speed = processed_count / elapsed if elapsed > 0 else 0
                    progress.update(task, advance=len(batch_emb), speed=speed)

    except Exception as e:
        # å¦‚æœä»»ä½•ä¸€ä¸ªworkerçº¿ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼Œç¨‹åºä¼šåœ¨è¿™é‡Œä¸­æ–­
        console.print("[bold red]åµŒå…¥å‘é‡ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œç¨‹åºå·²ç»ˆæ­¢ã€‚[/bold red]")
        raise typer.Exit(code=1)

    # 3. æœ€åä¸€æ­¥ä¸åŸæ¥ç›¸åŒï¼šå°†æ‰€æœ‰ç»“æœæ‹¼æ¥èµ·æ¥
    return np.vstack(all_embeddings)


def process_anchor_batch_gpu(all_embeddings, anchor_batch, positive_map, pool_sizes, k_values: List[int], use_gpu: bool = True) -> dict:
    """
    å¤„ç†é”šç‚¹æ‰¹æ¬¡ï¼Œè®¡ç®—ä¸æ‰€æœ‰åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œå¹¶è¿”å›Recall@Kç»“æœã€‚
    ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ç›¸ä¼¼åº¦ã€‚
    """
    recalls = {}
    for pool_size in pool_sizes:
        recalls[pool_size] = {}
        for k in k_values:
            recalls[pool_size][k] = [0, 0]  # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„åˆ—è¡¨
    
    anchors = all_embeddings[anchor_batch]
    max_pool_size = max(pool_sizes)
    pool_size = max_pool_size - 1
    pools = []
    batch_size = len(anchor_batch)
    
    for i in range(batch_size):
        anchor_idx = anchor_batch[i]
        positive = positive_map[anchor_idx]
        positive_anchor_idx = random.choice(positive)
        
        while True:
            # Sample the random Pool
            candidate_indices = np.random.choice(len(all_embeddings), size=pool_size, replace=False)
            candidate_indices_set = set(candidate_indices)
            if positive_anchor_idx in candidate_indices_set or anchor_idx in candidate_indices_set:
                continue
            else:
                batch_pool = np.concatenate(([positive_anchor_idx], candidate_indices))
                pools.append(batch_pool)
                break

    pools = np.array(pools)
    embedding_pools = all_embeddings[pools] # size: (batch_size, pool_size, embedding_dim)
    anchor_emb = anchors[:, np.newaxis, :]  # size: (batch_size, 1, embedding_dim)
    

    anchor_emb_gpu = cp.asarray(anchor_emb)
    embedding_pools_gpu = cp.asarray(embedding_pools)
    similarities = cp.einsum('bij,bkj->bik', anchor_emb_gpu, embedding_pools_gpu)
    similarities = cp.squeeze(similarities, axis=1)  # size: (batch_size, pool_size)


        
    # è®¡ç®—Recall@K
    for pool_size in pool_sizes:
        top_indices = cp.argsort(cp.argsort(-similarities[:, :pool_size], axis=1), axis=1)[:, 0] + 1
        for k in k_values:
            success, total = 0, 0
            success = (top_indices <= k).sum()
            total = len(top_indices)
            assert success <= total, f"Success count {success} cannot be greater than total {total}."
            recalls[pool_size][k][0] += success
            recalls[pool_size][k][1] += total

    return recalls


@app.command()
def main(
    validation_dataset_pool_path: Path = typer.Argument(..., help="éªŒè¯é›†æ•°æ®æ± çš„è·¯å¾„ã€‚", exists=True, dir_okay=True),
    validation_positive_map_path: Path = typer.Argument(..., help="éªŒè¯é›†æ­£æ ·æœ¬æ˜ å°„.pklæ–‡ä»¶è·¯å¾„ã€‚", exists=True, file_okay=True),
    tei_endpoint: str = typer.Option("http://gpu1.damaoooo.com:8080", help="Text Embedding Inference (TEI) æœåŠ¡å™¨çš„URLã€‚"),
    ks_str: str = typer.Option("1,5,10,15,20,25,30,35,40,45,50", "--ks", "-k", help="è¦è¯„ä¼°çš„Kå€¼ï¼Œä»¥é€—å·åˆ†éš”ã€‚"),
    batch_size: int = typer.Option(128, "--batch-size", "-b", help="å‘é€åˆ°TEIæœåŠ¡å™¨çš„æ‰¹é‡å¤§å°ã€‚"),
    max_length: int = typer.Option(2048, "--max-length", help="å‘é€åˆ°TEIå‰ï¼Œå°†'æŒ‡ä»¤+æ–‡æœ¬'æ•´ä½“æˆªæ–­åˆ°çš„æœ€å¤§tokené•¿åº¦ã€‚"),
    eval_samples: int = typer.Option(187256, "--eval-samples", "-n", help="ç”¨äºè¯„ä¼°çš„éšæœºé”šç‚¹æ ·æœ¬æ•°é‡ã€‚"),
    embeddings_path: Path = typer.Option(None, "--embeddings-path", "-e", help="ç”¨äºä¿å­˜/åŠ è½½åµŒå…¥å‘é‡Numpyæ–‡ä»¶çš„è·¯å¾„ã€‚"),
    seed: int = typer.Option(42, "--seed", "-s", help="ç”¨äºè´Ÿé‡‡æ ·çš„éšæœºç§å­ã€‚"),
    use_gpu: bool = typer.Option(True, "--gpu/--no-gpu", help="æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ã€‚"),
    gpu_batch_size: int = typer.Option(512, "--gpu-batch-size", help="GPUæ‰¹é‡å¤„ç†çš„é”šç‚¹æ•°é‡ã€‚"),
):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„å‡½æ•°æ£€ç´¢æ€§èƒ½ (Recall@K)ï¼Œä½¿ç”¨TEIæœåŠ¡å™¨åŠ é€ŸåµŒå…¥ç”Ÿæˆï¼ŒGPUåŠ é€Ÿç›¸ä¼¼åº¦è®¡ç®—ã€‚
    """
    console.rule(f"[bold blue]å¼€å§‹ä½¿ç”¨TEIè¿›è¡Œæ¨¡å‹è¯„ä¼°[/bold blue]")
    set_seed(seed)
    
    # GPUå¯ç”¨æ€§æ£€æŸ¥
    if use_gpu and not GPU_AVAILABLE:
        console.print("[yellow]âš  è¯·æ±‚ä½¿ç”¨GPUä½†CuPyä¸å¯ç”¨ï¼Œå°†å›é€€åˆ°CPUè®¡ç®—[/yellow]")
        use_gpu = False
    
    if use_gpu:
        console.print(f"[green]ğŸš€ å°†ä½¿ç”¨GPUåŠ é€Ÿï¼Œæ‰¹é‡å¤§å°: {gpu_batch_size}[/green]")
    else:
        console.print("[blue]ğŸ’» ä½¿ç”¨CPUè®¡ç®—[/blue]")
    
    # --- 1. åŠ è½½æ•°æ®å’ŒTokenizer ---
    logging.info("æ­£åœ¨åŠ è½½æ•°æ®å’ŒTokenizer...")
    validation_dataset = load_from_disk(str(validation_dataset_pool_path))
    with open(validation_positive_map_path, 'rb') as f:
        positive_map = pickle.load(f)
    # åŠ è½½Tokenizerç”¨äºå®¢æˆ·ç«¯æˆªæ–­
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-Embedding-0.6B", trust_remote_code=True)


    # --- 2. ç”Ÿæˆæˆ–åŠ è½½æ‰€æœ‰åµŒå…¥å‘é‡ ---
    if embeddings_path and embeddings_path.exists():
        logging.info(f"æ­£åœ¨ä» [cyan]{embeddings_path}[/cyan] åŠ è½½å·²ç¼“å­˜çš„åµŒå…¥å‘é‡...")
        all_embeddings = np.load(embeddings_path)
        logging.info(f"åµŒå…¥å‘é‡åŠ è½½å®Œæ¯•ï¼Œå½¢çŠ¶ä¸º: [green]{all_embeddings.shape}[/green]")
    else:
        instruction = "Represent this LLVM IR for searching for similar functions:"
        all_embeddings = generate_embeddings_with_tei(validation_dataset, batch_size, instruction, tei_endpoint, tokenizer, max_length)
        logging.info(f"åµŒå…¥å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œå½¢çŠ¶ä¸º: [green]{all_embeddings.shape}[/green]")
        
        if embeddings_path:
            logging.info(f"æ­£åœ¨å°†æ–°ç”Ÿæˆçš„åµŒå…¥å‘é‡ç¼“å­˜åˆ° [cyan]{embeddings_path}[/cyan]...")
            embeddings_path.parent.mkdir(parents=True, exist_ok=True)
            np.save(embeddings_path, all_embeddings)
            logging.info("ç¼“å­˜å®Œæˆã€‚")

    # --- 3. GPUå†…å­˜é¢„å¤„ç† ---
    if use_gpu:
        logging.info("æ­£åœ¨å°†åµŒå…¥å‘é‡è½¬ç§»åˆ°GPU...")
        all_embeddings_gpu = cp.asarray(all_embeddings)
        logging.info(f"GPUå†…å­˜ä½¿ç”¨: {all_embeddings_gpu.nbytes / (1024**3):.2f} GB")
    else:
        all_embeddings_gpu = None


    # --- 4. è®¾ç½®è¯„ä¼°å‚æ•° ---
    pool_sizes = [2**i for i in range(1, 14)] + [100, 10000]
    # Sort it
    pool_sizes = sorted(pool_sizes)
    k_values = sorted([int(k.strip()) for k in ks_str.split(',')])
    max_k = max(k_values)
    results = {}
    
    all_possible_anchors = list(positive_map.keys())
    if eval_samples > 0 and eval_samples < len(all_possible_anchors):
        logging.info(f"å°†ä» {len(all_possible_anchors):,} ä¸ªå¯èƒ½çš„é”šç‚¹ä¸­éšæœºé‡‡æ · [yellow]{eval_samples:,}[/yellow] ä¸ªè¿›è¡Œè¯„ä¼°...")
        anchors_to_evaluate = random.sample(all_possible_anchors, eval_samples)
    else:
        logging.info(f"å°†è¯„ä¼°æ‰€æœ‰ {len(all_possible_anchors):,} ä¸ªé”šç‚¹...")
        anchors_to_evaluate = all_possible_anchors


    # --- 5. å¯¹ä¸åŒçš„æ± å¤§å°è¿›è¡Œè¯„ä¼° ---
    logging.info("å¼€å§‹å¯¹ä¸åŒæ± å¤§å°è¿›è¡Œæ‰¹é‡GPUåŠ é€Ÿè¯„ä¼°...")
    
    temp_results = {}
    for pool_size in pool_sizes:
        temp_results[pool_size] = {k: [0, 0] for k in k_values}
    
    
    # ä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ¡æ˜¾ç¤ºè¯„ä¼°é€Ÿåº¦
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        TextColumn("â€¢"),
        MofNCompleteColumn(),
        TextColumn("â€¢"),
        TextColumn("[cyan]{task.fields[speed]:.1f} é”šç‚¹/ç§’"),
        TextColumn("â€¢"),
        TimeRemainingColumn(),
    ) as progress:
        task = progress.add_task(
            "æ­£åœ¨è¯„ä¼°...",
            total=len(anchors_to_evaluate),
            speed=0.0
        )
        
        processed_anchors = 0
        for i in range(0, len(anchors_to_evaluate), gpu_batch_size):
            anchor_batch = anchors_to_evaluate[i:i + gpu_batch_size]
            result = process_anchor_batch_gpu(
                all_embeddings_gpu if use_gpu else all_embeddings,
                anchor_batch,
                positive_map,
                pool_sizes,
                k_values,
                use_gpu=use_gpu
            )
            # ç´¯åŠ ç»“æœ
            for pool_size in pool_sizes:
                for k in k_values:
                    temp_results[pool_size][k][0] += result[pool_size][k][0]
                    temp_results[pool_size][k][1] += result[pool_size][k][1]
            
            # æ›´æ–°è¿›åº¦å’Œé€Ÿåº¦
            processed_anchors += len(anchor_batch)
            elapsed = progress.tasks[0].elapsed or 0.001
            speed = processed_anchors / elapsed if elapsed > 0 else 0
            progress.update(task, advance=len(anchor_batch), speed=speed)
                
    # å°†ç»“æœè½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    for pool_size in pool_sizes:
        results[pool_size] = {f"Recall@{k}": temp_results[pool_size][k][0] / temp_results[pool_size][k][1] if temp_results[pool_size][k][1] > 0 else 0 for k in k_values}

    # --- 6. æ‰“å°ç»“æœ ---
    console.rule("[bold green]è¯„ä¼°ç»“æœ[/bold green]")
    table = Table(title="Recall@K åœ¨ä¸åŒå¤§å°çš„æ£€ç´¢æ± ä¸­çš„è¡¨ç°")
    table.add_column("Pool Size", justify="right", style="cyan")
    for k in k_values:
        table.add_column(f"Recall@{k}", justify="right", style="magenta")

    for pool_size, recalls in results.items():
        row_data = [f"{pool_size:,}"] + [f"{recalls[f'Recall@{k}']:.4f}" for k in k_values]
        table.add_row(*row_data)
        
    console.print(table)


if __name__ == "__main__":
    app()
