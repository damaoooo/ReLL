import logging
import pickle
import random
from pathlib import Path
from typing import List

import numpy as np
import requests
import typer
from rich.console import Console
from rich.logging import RichHandler
from rich.progress import track
from rich.table import Table
# --- æ ¸å¿ƒä¿®æ­£ï¼šå¼•å…¥ AutoTokenizer ---
from transformers import AutoTokenizer, set_seed
from datasets import load_from_disk


# --- è®¾ç½® Rich å’Œ Typer ---
logging.basicConfig(
    level="INFO",
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True, markup=True)],
)
app = typer.Typer(pretty_exceptions_show_locals=False)
console = Console()

# --- GPUåŠ é€Ÿæ”¯æŒ ---
try:
    import cupy as cp
    GPU_AVAILABLE = True
    console.print("[green]âœ“ GPUåŠ é€Ÿå·²å¯ç”¨ (CuPy)[/green]")
except ImportError:
    GPU_AVAILABLE = False
    console.print("[yellow]âš  æœªå®‰è£…CuPyï¼Œå°†ä½¿ç”¨CPUè®¡ç®—[/yellow]")


def generate_embeddings_with_tei(dataset, batch_size: int, instruction: str, tei_endpoint: str, tokenizer, max_length: int) -> np.ndarray:
    """
    ä½¿ç”¨Text Embedding Inference (TEI)æœåŠ¡å™¨ä¸ºæ•´ä¸ªæ•°æ®é›†ç”ŸæˆåµŒå…¥å‘é‡ã€‚
    åœ¨å‘é€å‰ï¼Œå…ˆåœ¨å®¢æˆ·ç«¯è¿›è¡Œç²¾ç¡®çš„ã€ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„æˆªæ–­ã€‚
    """
    all_embeddings = []
    session = requests.Session()

    for i in track(range(0, len(dataset), batch_size), description="æ­£åœ¨é€šè¿‡TEIç”ŸæˆåµŒå…¥å‘é‡..."):
        batch_texts = dataset[i : i + batch_size]['text']
        
        # --- æ ¸å¿ƒä¿®æ­£ï¼šåœ¨å®¢æˆ·ç«¯è¿›è¡Œç²¾ç¡®æˆªæ–­ ---
        # 1. é¦–å…ˆå°†æŒ‡ä»¤å’Œå‡½æ•°æ–‡æœ¬æ‹¼æ¥ï¼Œå®Œå…¨æ¨¡æ‹Ÿæ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°çš„è¾“å…¥
        instructed_texts = [instruction + text for text in batch_texts]

        # 2. ä½¿ç”¨Tokenizerå¯¹æ‹¼æ¥åçš„å®Œæ•´æ–‡æœ¬è¿›è¡Œæˆªæ–­
        truncated_inputs = tokenizer(
            instructed_texts,
            truncation=True,
            max_length=max_length, # ä½¿ç”¨æˆ‘ä»¬æŒ‡å®šçš„é•¿åº¦ï¼Œä¾‹å¦‚2048
            padding=False, # æˆ‘ä»¬ä¸éœ€è¦paddingï¼Œåªéœ€è¦æˆªæ–­
        )
        # 3. å°†æˆªæ–­åçš„token IDsè§£ç å›æ–‡æœ¬
        final_texts_to_send = tokenizer.batch_decode(truncated_inputs['input_ids'], skip_special_tokens=True)
        
        # 4. å‘é€ç»™TEIï¼Œæ­¤æ—¶ä¸å†éœ€è¦TEIè¿›è¡Œæˆªæ–­
        payload = {"inputs": final_texts_to_send}
        
        try:
            response = session.post(f"{tei_endpoint}/embed", json=payload, timeout=60)
            response.raise_for_status()
            
            batch_embeddings = np.array(response.json(), dtype=np.float32)
            all_embeddings.append(batch_embeddings)
            
        except requests.exceptions.RequestException as e:
            console.print(f"[bold red]é”™è¯¯: TEIè¯·æ±‚å¤±è´¥ at batch {i}-{i+batch_size}: {e}[/bold red]")
            console.print("è¯·ç¡®ä¿æ‚¨çš„TEIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”endpointåœ°å€æ­£ç¡®ã€‚")
            raise typer.Exit(code=1)

    return np.vstack(all_embeddings)


def process_anchor_batch_gpu(all_embeddings, anchor_batch, positive_map, pool_sizes, k_values: List[int], use_gpu: bool = True) -> dict:
    """
    å¤„ç†é”šç‚¹æ‰¹æ¬¡ï¼Œè®¡ç®—ä¸æ‰€æœ‰åµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œå¹¶è¿”å›Recall@Kç»“æœã€‚
    ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ç›¸ä¼¼åº¦ã€‚
    """
    recalls = {}
    for pool_size in pool_sizes:
        recalls[pool_size] = {}
        for k in k_values:
            recalls[pool_size][k] = [0, 0]  # æ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„åˆ—è¡¨
    
    anchors = all_embeddings[anchor_batch]
    max_pool_size = max(pool_sizes)
    pool_size = max_pool_size - 1
    pools = []
    batch_size = len(anchor_batch)
    
    for i in range(batch_size):
        anchor_idx = anchor_batch[i]
        positive = positive_map[anchor_idx]
        positive_anchor_idx = random.choice(positive)
        
        while True:
            # Sample the random Pool
            candidate_indices = np.random.choice(len(all_embeddings), size=pool_size, replace=False)
            candidate_indices_set = set(candidate_indices)
            if positive_anchor_idx in candidate_indices_set or anchor_idx in candidate_indices_set:
                continue
            else:
                batch_pool = np.concatenate(([positive_anchor_idx], candidate_indices))
                pools.append(batch_pool)
                break

    pools = np.array(pools)
    embedding_pools = all_embeddings[pools] # size: (batch_size, pool_size, embedding_dim)
    anchor_emb = anchors[:, np.newaxis, :]  # size: (batch_size, 1, embedding_dim)
    

    anchor_emb_gpu = cp.asarray(anchor_emb)
    embedding_pools_gpu = cp.asarray(embedding_pools)
    similarities = cp.einsum('bij,bkj->bik', anchor_emb_gpu, embedding_pools_gpu)
    similarities = cp.squeeze(similarities, axis=1)  # size: (batch_size, pool_size)


        
    # è®¡ç®—Recall@K
    for pool_size in pool_sizes:
        top_indices = cp.argsort(cp.argsort(-similarities[:, :pool_size], axis=1), axis=1)[:, 0] + 1
        for k in k_values:
            success, total = 0, 0
            success = (top_indices <= k).sum()
            total = len(top_indices)
            assert success <= total, f"Success count {success} cannot be greater than total {total}."
            recalls[pool_size][k][0] += success
            recalls[pool_size][k][1] += total

    return recalls


@app.command()
def main(
    validation_dataset_pool_path: Path = typer.Argument(..., help="éªŒè¯é›†æ•°æ®æ± çš„è·¯å¾„ã€‚", exists=True, dir_okay=True),
    validation_positive_map_path: Path = typer.Argument(..., help="éªŒè¯é›†æ­£æ ·æœ¬æ˜ å°„.pklæ–‡ä»¶è·¯å¾„ã€‚", exists=True, file_okay=True),
    tei_endpoint: str = typer.Option("http://gpu1.damaoooo.com:8080", help="Text Embedding Inference (TEI) æœåŠ¡å™¨çš„URLã€‚"),
    ks_str: str = typer.Option("1,5,10,15,20,25,30,35,40,45,50", "--ks", "-k", help="è¦è¯„ä¼°çš„Kå€¼ï¼Œä»¥é€—å·åˆ†éš”ã€‚"),
    batch_size: int = typer.Option(128, "--batch-size", "-b", help="å‘é€åˆ°TEIæœåŠ¡å™¨çš„æ‰¹é‡å¤§å°ã€‚"),
    max_length: int = typer.Option(2048, "--max-length", help="å‘é€åˆ°TEIå‰ï¼Œå°†'æŒ‡ä»¤+æ–‡æœ¬'æ•´ä½“æˆªæ–­åˆ°çš„æœ€å¤§tokené•¿åº¦ã€‚"),
    eval_samples: int = typer.Option(187256, "--eval-samples", "-n", help="ç”¨äºè¯„ä¼°çš„éšæœºé”šç‚¹æ ·æœ¬æ•°é‡ã€‚"),
    embeddings_path: Path = typer.Option(None, "--embeddings-path", "-e", help="ç”¨äºä¿å­˜/åŠ è½½åµŒå…¥å‘é‡Numpyæ–‡ä»¶çš„è·¯å¾„ã€‚"),
    seed: int = typer.Option(42, "--seed", "-s", help="ç”¨äºè´Ÿé‡‡æ ·çš„éšæœºç§å­ã€‚"),
    use_gpu: bool = typer.Option(True, "--gpu/--no-gpu", help="æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ã€‚"),
    gpu_batch_size: int = typer.Option(512, "--gpu-batch-size", help="GPUæ‰¹é‡å¤„ç†çš„é”šç‚¹æ•°é‡ã€‚"),
):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„å‡½æ•°æ£€ç´¢æ€§èƒ½ (Recall@K)ï¼Œä½¿ç”¨TEIæœåŠ¡å™¨åŠ é€ŸåµŒå…¥ç”Ÿæˆï¼ŒGPUåŠ é€Ÿç›¸ä¼¼åº¦è®¡ç®—ã€‚
    """
    console.rule(f"[bold blue]å¼€å§‹ä½¿ç”¨TEIè¿›è¡Œæ¨¡å‹è¯„ä¼°[/bold blue]")
    set_seed(seed)
    
    # GPUå¯ç”¨æ€§æ£€æŸ¥
    if use_gpu and not GPU_AVAILABLE:
        console.print("[yellow]âš  è¯·æ±‚ä½¿ç”¨GPUä½†CuPyä¸å¯ç”¨ï¼Œå°†å›é€€åˆ°CPUè®¡ç®—[/yellow]")
        use_gpu = False
    
    if use_gpu:
        console.print(f"[green]ğŸš€ å°†ä½¿ç”¨GPUåŠ é€Ÿï¼Œæ‰¹é‡å¤§å°: {gpu_batch_size}[/green]")
    else:
        console.print("[blue]ğŸ’» ä½¿ç”¨CPUè®¡ç®—[/blue]")
    
    # --- 1. åŠ è½½æ•°æ®å’ŒTokenizer ---
    logging.info("æ­£åœ¨åŠ è½½æ•°æ®å’ŒTokenizer...")
    validation_dataset = load_from_disk(str(validation_dataset_pool_path))
    with open(validation_positive_map_path, 'rb') as f:
        positive_map = pickle.load(f)
    # åŠ è½½Tokenizerç”¨äºå®¢æˆ·ç«¯æˆªæ–­
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-Embedding-0.6B", trust_remote_code=True)


    # --- 2. ç”Ÿæˆæˆ–åŠ è½½æ‰€æœ‰åµŒå…¥å‘é‡ ---
    if embeddings_path and embeddings_path.exists():
        logging.info(f"æ­£åœ¨ä» [cyan]{embeddings_path}[/cyan] åŠ è½½å·²ç¼“å­˜çš„åµŒå…¥å‘é‡...")
        all_embeddings = np.load(embeddings_path)
        logging.info(f"åµŒå…¥å‘é‡åŠ è½½å®Œæ¯•ï¼Œå½¢çŠ¶ä¸º: [green]{all_embeddings.shape}[/green]")
    else:
        instruction = "Represent this LLVM IR for searching for similar functions:"
        all_embeddings = generate_embeddings_with_tei(validation_dataset, batch_size, instruction, tei_endpoint, tokenizer, max_length)
        logging.info(f"åµŒå…¥å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œå½¢çŠ¶ä¸º: [green]{all_embeddings.shape}[/green]")
        
        if embeddings_path:
            logging.info(f"æ­£åœ¨å°†æ–°ç”Ÿæˆçš„åµŒå…¥å‘é‡ç¼“å­˜åˆ° [cyan]{embeddings_path}[/cyan]...")
            embeddings_path.parent.mkdir(parents=True, exist_ok=True)
            np.save(embeddings_path, all_embeddings)
            logging.info("ç¼“å­˜å®Œæˆã€‚")

    # --- 3. GPUå†…å­˜é¢„å¤„ç† ---
    if use_gpu:
        logging.info("æ­£åœ¨å°†åµŒå…¥å‘é‡è½¬ç§»åˆ°GPU...")
        all_embeddings_gpu = cp.asarray(all_embeddings)
        logging.info(f"GPUå†…å­˜ä½¿ç”¨: {all_embeddings_gpu.nbytes / (1024**3):.2f} GB")
    else:
        all_embeddings_gpu = None


    # --- 4. è®¾ç½®è¯„ä¼°å‚æ•° ---
    pool_sizes = [2**i for i in range(1, 14)] + [100, 10000]
    # Sort it
    pool_sizes = sorted(pool_sizes)
    k_values = sorted([int(k.strip()) for k in ks_str.split(',')])
    max_k = max(k_values)
    results = {}
    
    all_possible_anchors = list(positive_map.keys())
    if eval_samples > 0 and eval_samples < len(all_possible_anchors):
        logging.info(f"å°†ä» {len(all_possible_anchors):,} ä¸ªå¯èƒ½çš„é”šç‚¹ä¸­éšæœºé‡‡æ · [yellow]{eval_samples:,}[/yellow] ä¸ªè¿›è¡Œè¯„ä¼°...")
        anchors_to_evaluate = random.sample(all_possible_anchors, eval_samples)
    else:
        logging.info(f"å°†è¯„ä¼°æ‰€æœ‰ {len(all_possible_anchors):,} ä¸ªé”šç‚¹...")
        anchors_to_evaluate = all_possible_anchors


    # --- 5. å¯¹ä¸åŒçš„æ± å¤§å°è¿›è¡Œè¯„ä¼° ---
    logging.info("å¼€å§‹å¯¹ä¸åŒæ± å¤§å°è¿›è¡Œæ‰¹é‡GPUåŠ é€Ÿè¯„ä¼°...")
    
    temp_results = {}
    for pool_size in pool_sizes:
        temp_results[pool_size] = {k: [0, 0] for k in k_values}
    
    
    for i in track(range(0, len(anchors_to_evaluate), gpu_batch_size), description="æ­£åœ¨è¯„ä¼°..."):
        anchor_batch = anchors_to_evaluate[i:i + gpu_batch_size]
        result = process_anchor_batch_gpu(
            all_embeddings_gpu if use_gpu else all_embeddings,
            anchor_batch,
            positive_map,
            pool_sizes,
            k_values,
            use_gpu=use_gpu
        )
        # ç´¯åŠ ç»“æœ
        for pool_size in pool_sizes:
            for k in k_values:
                temp_results[pool_size][k][0] += result[pool_size][k][0]
                temp_results[pool_size][k][1] += result[pool_size][k][1]
                
    # å°†ç»“æœè½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    for pool_size in pool_sizes:
        results[pool_size] = {f"Recall@{k}": temp_results[pool_size][k][0] / temp_results[pool_size][k][1] if temp_results[pool_size][k][1] > 0 else 0 for k in k_values}

    # --- 6. æ‰“å°ç»“æœ ---
    console.rule("[bold green]è¯„ä¼°ç»“æœ[/bold green]")
    table = Table(title="Recall@K åœ¨ä¸åŒå¤§å°çš„æ£€ç´¢æ± ä¸­çš„è¡¨ç°")
    table.add_column("Pool Size", justify="right", style="cyan")
    for k in k_values:
        table.add_column(f"Recall@{k}", justify="right", style="magenta")

    for pool_size, recalls in results.items():
        row_data = [f"{pool_size:,}"] + [f"{recalls[f'Recall@{k}']:.4f}" for k in k_values]
        table.add_row(*row_data)
        
    console.print(table)


if __name__ == "__main__":
    app()
